# CLIP Integration Guide

## ĞĞ±Ğ·Ğ¾Ñ€ (Overview)

Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ CLIP (Contrastive Language-Image Pre-training) Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ AI, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½Ğ¾Ğº Ğ´Ğ»Ñ ÑĞ»Ğ°Ğ¹Ğ´Ğ¾Ğ².

**CLIP integration adds AI-powered semantic image search, dramatically improving image relevance for slides.**

---

## Ğ§Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¾ÑÑŒ (What Changed)

### 1. ĞĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ (New Dependencies)

Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ² `requirements.txt`:
```
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.30.0
sentence-transformers>=2.2.0
Pillow>=10.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
```

### 2. ĞĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ (New Modules)

#### `services/clip_client.py`
Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ CLIP Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ:
- `is_clip_available()` - Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ CLIP
- `get_text_embedding(text)` - Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°
- `get_image_embedding(image_url)` - Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
- `compute_similarity(emb1, emb2)` - Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°
- Ğ’ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸

**Features:**
- Lazy model loading (one-time initialization)
- In-memory embedding cache (up to 1000 entries)
- Graceful fallback if CLIP unavailable
- Using `clip-ViT-B-32` model (balanced speed/quality)

#### `services/image_matcher.py`
Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹:
- `pick_best_image_for_slide()` - Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ°
- `rank_images_by_relevance()` - Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ²
- `get_similarity_for_image()` - Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° (Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸)

**Algorithm:**
1. Combine slide title + content into semantic context
2. Get CLIP embedding for context
3. Get embeddings for all candidate images (via descriptions)
4. Compute cosine similarity for each candidate
5. Return image with highest similarity above threshold (0.25 default)
6. If all below threshold, return None (avoids poor matches)

### 3. Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² app.py

#### Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹ (Imports)
```python
# CLIP services for semantic image matching
try:
    from services.clip_client import is_clip_available, get_text_embedding
    from services.image_matcher import pick_best_image_for_slide as clip_pick_best_image
    CLIP_ENABLED = True
except ImportError:
    CLIP_ENABLED = False
    clip_pick_best_image = None
```

#### ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ `search_image_for_slide()`
Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°:

**Ğ­Ñ‚Ğ°Ğ¿ 1: CLIP-enhanced search (ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½)**
- ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ 10-15 ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼
- ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ°Ñ‚Ñ‡Ğ¸Ğ½Ğ³ Ñ‡ĞµÑ€ĞµĞ· CLIP
- Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ

**Ğ­Ñ‚Ğ°Ğ¿ 2: Fallback (ĞµÑĞ»Ğ¸ CLIP Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ¸Ğ»Ğ¸ Ğ½Ğµ Ğ½Ğ°ÑˆÑ‘Ğ» Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞµ)**
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼
- Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ

---

## Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° (Installation)

### 1. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸

```bash
pip install -r requirements.txt
```

**Ğ’Ğ°Ğ¶Ğ½Ğ¾:** Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° PyTorch Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ½ÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ (~2GB ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ).

**Ğ”Ğ»Ñ GPU Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾):**
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### 2. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº

ĞŸÑ€Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ Ğ·Ğ°Ğ¿ÑƒÑĞºĞµ CLIP Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞºĞ°Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ (~600MB):
```
ğŸ”„ Loading CLIP model (this may take a minute on first run)...
âœ… CLIP model loaded successfully
   â†’ Model: clip-ViT-B-32
   â†’ Embedding dimension: 512
```

ĞœĞ¾Ğ´ĞµĞ»ÑŒ ĞºÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ² `~/.cache/torch/sentence_transformers/`.

---

## Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (Usage)

### ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

CLIP Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞĞ¸ĞºĞ°ĞºĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² API Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ!

ĞŸÑ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ»Ğ¾Ğ³Ğ°Ñ… Ğ±ÑƒĞ´ĞµÑ‚:
```
ğŸ” Searching image for slide: 'Revenue Growth Analysis'
  ğŸ¯ Keywords extracted: ['revenue', 'growth', 'analysis']
  ğŸ¤– Using CLIP semantic matching for better relevance
  ğŸ“Š Found 15 candidates, applying CLIP ranking...
     [1] Business chart showing financial growth â†’ 0.782
     [2] Professional business team in office   â†’ 0.543
     [3] Mountain landscape with sunset         â†’ 0.234
  âœ… CLIP selected best match: https://images.pexels.com/...
```

### ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ° (Threshold Configuration)

Ğ’ `app.py`, Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ `search_image_for_slide()`:
```python
best_image = clip_pick_best_image(
    slide_title=slide_title,
    slide_content=slide_content,
    image_candidates=candidates,
    exclude_images=exclude_images,
    similarity_threshold=0.25  # Ğ˜Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ·Ğ´ĞµÑÑŒ
)
```

**Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸:**
- `0.15-0.25` - Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼ÑĞ³ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ÑÑ)
- `0.25-0.35` - ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ (Ğ±Ğ°Ğ»Ğ°Ğ½Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°/ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°)
- `0.35-0.50` - ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‡ĞµĞ½ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ)

### Ğ ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

```python
from services.clip_client import get_text_embedding, compute_similarity

# ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸
text_emb = get_text_embedding("Financial growth chart")
image_desc_emb = get_text_embedding("Business revenue statistics")

# Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾
similarity = compute_similarity(text_emb, image_desc_emb)
print(f"Similarity: {similarity:.3f}")  # 0.0 - 1.0
```

---

## Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (Testing)

### Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²ÑĞµÑ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²

```bash
# Ğ¢ĞµÑÑ‚Ñ‹ CLIP client
python -m pytest tests/test_clip_client.py -v

# Ğ¢ĞµÑÑ‚Ñ‹ image matcher
python -m pytest tests/test_image_matcher.py -v

# Ğ’ÑĞµ Ñ‚ĞµÑÑ‚Ñ‹
python -m pytest tests/ -v
```

### Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚

```bash
# Ğ¢ĞµÑÑ‚ CLIP matcher Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ¼
python -m services.image_matcher
```

Ğ’Ñ‹Ğ²Ğ¾Ğ´:
```
============================================================
Testing CLIP Image Matcher
============================================================

Slide: 'Revenue Growth Analysis'
Content: Our Q4 2024 financial results show a 45% increase in revenue...

Testing semantic matching...

  ğŸ¤– CLIP semantic matching for: 'Revenue Growth Analysis'
     [1] Business chart showing financial growth â†’ 0.823
     [2] Professional business team working      â†’ 0.542
     [3] Beautiful mountain landscape            â†’ 0.187

âœ… Best match selected:
   URL: https://example.com/chart.jpg
   Description: Business chart showing financial growth

============================================================
```

---

## ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ (Performance)

### ĞšÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

CLIP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:

1. **ĞšÑÑˆ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²** (in-memory)
   - Ğ”Ğ¾ 1000 Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹
   - ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ€Ñ‹Ñ…
   - ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°: `clip_client.get_cache_stats()`

2. **ĞšÑÑˆ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸** (Ğ´Ğ¸ÑĞº)
   - ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·
   - Ğ¥Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑÑ Ğ² `~/.cache/torch/sentence_transformers/`
   - Ğ›ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° (Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸)

### Ğ’Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ

**ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº (Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸):**
- Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ~30-60 ÑĞµĞºÑƒĞ½Ğ´
- ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ»Ğ°Ğ¹Ğ´Ğ°: ~2-3 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ (15 ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ²)

**ĞŸĞ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¸:**
- ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ»Ğ°Ğ¹Ğ´Ğ°: ~1-2 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ (Ñ ĞºÑÑˆĞµĞ¼)
- Ğ‘ĞµĞ· ĞºÑÑˆĞ°: ~2-3 ÑĞµĞºÑƒĞ½Ğ´Ñ‹

**ĞĞ° GPU (ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½):**
- ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ»Ğ°Ğ¹Ğ´Ğ°: ~0.5-1 ÑĞµĞºÑƒĞ½Ğ´Ğ°

### ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸:

1. Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ²:
```python
candidate_count = 10  # Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 15
```

2. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ GPU (ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½):
```python
# PyTorch Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ GPU ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°: torch.cuda.is_available()
```

---

## Graceful Degradation (ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº)

CLIP Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ñ ÑƒĞ¼Ğ½Ñ‹Ğ¼ fallback'Ğ¾Ğ¼:

### Ğ¡Ñ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹ 1: CLIP Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ (Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹)
```
âš ï¸ CLIP services not available: No module named 'torch'
   â†’ Install dependencies: pip install torch sentence-transformers
```
â†’ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼

### Ğ¡Ñ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹ 2: ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ÑÑ
```
âŒ Failed to load CLIP model: Connection timeout
```
â†’ ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ fallback Ğ½Ğ° keyword search

### Ğ¡Ñ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹ 3: Ğ’ÑĞµ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ñ‹ Ğ½Ğ¸Ğ¶Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°
```
âš ï¸ CLIP found no suitable match (below threshold or all excluded)
ğŸ” Falling back to traditional keyword search
```
â†’ ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº

### Ğ¡Ñ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹ 4: ĞĞµÑ‚ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ²
```
âš ï¸ No image candidates found
ğŸ” Falling back to traditional keyword search
```
â†’ Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ fallback Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹

**Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:** Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ĞĞ˜ĞšĞĞ“Ğ”Ğ Ğ½Ğµ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¸Ğ·-Ğ·Ğ° CLIP, Ğ²ÑĞµĞ³Ğ´Ğ° ĞµÑÑ‚ÑŒ fallback!

---

## ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° (Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  app.py (create_presentation)                       â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  search_image_for_slide()                    â”‚  â”‚
â”‚  â”‚                                               â”‚  â”‚
â”‚  â”‚  1. Extract keywords from slide              â”‚  â”‚
â”‚  â”‚  2. Check if CLIP available                  â”‚  â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚  â”‚     â”‚ YES â†’ CLIP Path         â”‚              â”‚  â”‚
â”‚  â”‚     â”‚                          â”‚              â”‚  â”‚
â”‚  â”‚     â”‚ â€¢ Fetch 15 candidates    â”‚              â”‚  â”‚
â”‚  â”‚     â”‚ â€¢ Apply semantic match   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚     â”‚ â€¢ Pick best by CLIP      â”‚         â”‚   â”‚  â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚  â”‚
â”‚  â”‚                                          â”‚   â”‚  â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚  â”‚
â”‚  â”‚     â”‚ NO â†’ Keyword Path       â”‚         â”‚   â”‚  â”‚
â”‚  â”‚     â”‚                          â”‚         â”‚   â”‚  â”‚
â”‚  â”‚     â”‚ â€¢ Traditional search     â”‚         â”‚   â”‚  â”‚
â”‚  â”‚     â”‚ â€¢ First matching image   â”‚         â”‚   â”‚  â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚  â”‚
â”‚  â”‚                                          â”‚   â”‚  â”‚
â”‚  â”‚  3. Return (image_data, url, query)     â”‚   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  services/image_matcher.py                            â”‚
â”‚                                                        â”‚
â”‚  pick_best_image_for_slide()                          â”‚
â”‚  â”‚                                                     â”‚
â”‚  â”œâ”€ 1. Combine slide title + content                  â”‚
â”‚  â”œâ”€ 2. Get CLIP embedding for context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”œâ”€ 3. For each candidate:                        â”‚   â”‚
â”‚  â”‚    â€¢ Get embedding for description             â”‚   â”‚
â”‚  â”‚    â€¢ Compute similarity                        â”‚   â”‚
â”‚  â”œâ”€ 4. Sort by similarity (descending)            â”‚   â”‚
â”‚  â”œâ”€ 5. Filter by threshold (0.25)                 â”‚   â”‚
â”‚  â””â”€ 6. Return best match or None                  â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  services/clip_client.py                              â”‚
â”‚                                                        â”‚
â”‚  â€¢ _clip_model (lazy loaded, singleton)               â”‚
â”‚  â€¢ _embedding_cache (dict, max 1000)                  â”‚
â”‚                                                        â”‚
â”‚  get_text_embedding(text)                             â”‚
â”‚  â”‚                                                     â”‚
â”‚  â”œâ”€ 1. Check cache                                    â”‚
â”‚  â”œâ”€ 2. If miss: encode with CLIP                      â”‚
â”‚  â”œâ”€ 3. L2 normalize                                   â”‚
â”‚  â”œâ”€ 4. Cache result                                   â”‚
â”‚  â””â”€ 5. Return numpy array (512,)                      â”‚
â”‚                                                        â”‚
â”‚  compute_similarity(emb1, emb2)                       â”‚
â”‚  â””â”€ Dot product (embeddings are normalized)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ĞÑ‚Ğ»Ğ°Ğ´ĞºĞ° (Debugging)

### ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ CLIP

```python
from services.clip_client import is_clip_available

if is_clip_available():
    print("âœ… CLIP ready!")
else:
    print("âŒ CLIP not available")
```

### ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ĞºÑÑˆ

```python
from services.clip_client import get_cache_stats

stats = get_cache_stats()
print(f"Cache size: {stats['size']}/{stats['max_size']}")
```

### ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ ĞºÑÑˆ

```python
from services.clip_client import clear_cache
clear_cache()
```

### ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ

```python
from services.image_matcher import get_similarity_for_image

score = get_similarity_for_image(
    slide_title="Revenue Growth",
    slide_content="Financial results Q4",
    image_description="Business chart with increasing revenue"
)
print(f"Similarity: {score:.3f}")
```

---

## FAQ

### Q: ĞÑƒĞ¶ĞµĞ½ Ğ»Ğ¸ GPU Ğ´Ğ»Ñ CLIP?
**A:** ĞĞµÑ‚, CPU Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾. GPU ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ (~2-3x Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ), Ğ½Ğ¾ Ğ½Ğµ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ĞµĞ½.

### Q: Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑÑ‚Ğ° Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ?
**A:** ~600MB Ğ´Ğ»Ñ `clip-ViT-B-32` Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.

### Q: ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ÑƒÑ CLIP Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ?
**A:** Ğ”Ğ°, Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚Ğµ Ğ² `services/clip_client.py`:
```python
_clip_model = SentenceTransformer('clip-ViT-L-14')  # Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ
# Ğ¸Ğ»Ğ¸
_clip_model = SentenceTransformer('clip-ViT-B-16')  # ĞšĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ
```

### Q: Ğ§Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ ĞµÑĞ»Ğ¸ CLIP ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹?
**A:** 
1. Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚Ğµ `candidate_count` Ñ 15 Ğ´Ğ¾ 10
2. Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ÑŒÑ‚Ğµ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ `similarity_threshold` Ğ´Ğ¾ 0.3-0.4
3. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ GPU ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½

### Q: ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ CLIP Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ "Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ" Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ?
**A:**
1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² (Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ğ»Ğ¾Ñ…Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
2. Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ÑŒÑ‚Ğµ `similarity_threshold` (ÑÑ‚Ñ€Ğ¾Ğ¶Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ)
3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ slide_content (Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° = Ğ»ÑƒÑ‡ÑˆĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚)

### Q: ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğ¾Ñ‚ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ CLIP Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹?
**A:** Ğ”Ğ°, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ:
```bash
CLIP_ENABLED=false
```
Ğ˜Ğ»Ğ¸ Ğ² ĞºĞ¾Ğ´Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾:
```python
CLIP_ENABLED = False  # Ğ² app.py
```

---

## Roadmap (Ğ‘ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ)

### Planned Features:
1. **Image embedding caching** - ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² ÑĞ°Ğ¼Ğ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹)
2. **Fine-tuned threshold** - Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ° Ğ¿Ğ¾ feedback
3. **Multi-modal search** - ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
4. **Redis caching** - Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
5. **A/B testing** - ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ CLIP vs keyword Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹

### Contributions Welcome!
Ğ˜Ğ´ĞµĞ¸ Ğ¸ PR Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ÑÑ Ğ² Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°.

---

## Troubleshooting

### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: ModuleNotFoundError: No module named 'torch'
**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:**
```bash
pip install torch torchvision sentence-transformers
```

### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: CLIP model download timeout
**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:**
```bash
# Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('clip-ViT-B-32')"
```

### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: Out of memory Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:**
- Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (`clip-ViT-B-16`)
- Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ swap Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ

### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° CPU
**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:**
1. Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ `candidate_count`
2. Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¾)
3. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ GPU Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ CPU

---

## ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ°

ĞŸÑ€Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼:
1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
2. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Ñ‚ĞµÑÑ‚Ñ‹: `python -m pytest tests/ -v`
3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹: `pip list | grep -E "torch|sentence"`
4. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ issue Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹

---

**Ğ’ĞµÑ€ÑĞ¸Ñ:** 1.0.0  
**Ğ”Ğ°Ñ‚Ğ°:** 2024-12-21  
**ĞĞ²Ñ‚Ğ¾Ñ€:** AI SlideRush Team
